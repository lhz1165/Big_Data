大数据环境

```bash
export JAVA_HOME=/home/lai/java/jdk/jdk1.8.0_202
export JRE_HOME=${JAVA_HOME}/jre
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
export PATH=${JAVA_HOME}/bin:$PATH

export M2_HOME=/home/lai/java/maven/apache-maven-3.8.4
export PATH=${M2_HOME}/bin:$PATH

export SCALA_HOME=/home/lai/bigdata/scala/scala-2.12.13
export PATH=${SCALA_HOME}/bin:$PATH

export HADOOP_HOME=/home/lai/bigdata/hadoop/hadoop-3.3.0
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin

export HIVE_HOME=/home/lai/bigdata/hive/apache-hive-3.1.2-bin
export PATH=$PATH:$HIVE_HOME/bin
```

安装Hadoop和HIVE

HIVE配置

```shell
cd /home/lai/bigdata/hive/apache-hive-3.1.2-bin/conf
vi hive-site.xml
```

HIVE 配置文件

```xml
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://localhost:3306/hive</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>com.mysql.jdbc.Driver</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>root</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>root</value>
       <!-- 当前用户全设置成root -->
    </property>
        <property>
       <name>hadoop.http.staticuser.user</name>
       <value>root</value>
    </property>
 <!-- 当前用户全设置成不需要权限 -->
    <property>
        <name>dfs.webhdfs.enabled</name>
        <value>false</value>
    </property>
</configuration>
</configuration>
```

hive和hadoop jar同步

```shell
## 进入hadoop lib
cd /home/lai/bigdata/hadoop/hadoop-3.3.0/share/hadoop/common/lib/
## hadoop jar拷贝给hive
cp guava-27.0-jre.jar /home/lai/bigdata/hive/apache-hive-3.1.2-bin/lib/
## 进入hive lib
cd /home/lai/bigdata/hive/apache-hive-3.1.2-bin/lib/
## 删掉hive的
rm -rf guava-19.0.jar

```

初始化数据库

```shell
# 进入目录
cd /home/lai/bigdata/hive/apache-hive-3.1.2-bin
# 初始化
schematool -dbType mysql -initSchema
```

有可能需要给root权限

```mysql
#mysql5.7
# 将所有数据库的所有表的所有权限赋给hive用户，后面的hive是配置hive-site.xml中配置的连接密码
mysql> grant all on *.* to root@localhost identified by 'root';   
# 刷新mysql系统权限关系表
mysql> flush privileges;  
------------------------------------------------------------------------------------
# mysql8 
# grant 权限 on 数据库名.表名 to 用户@登录主机 identified by “用户密码”;
mysql>GRANT ALL PRIVILEGES ON *.* TO "root"@"%" IDENTIFIED BY "root" WITH GRANT OPTION;

mysql> flush privileges;
```



### Hadoop 伪分布式设置

##### ssh localhost 无密码登录

```shell
# 启动ssh服务(百度)

sudo apt-get install openssh-server
sudo vi /etc/ssh/sshd_config
ListenAddress localhost  # 只接受本地请求
PasswordAuthentication yes  # 允许密码登录
sudo service ssh restart

#创建sshkey
ssh-keygen -A
#启动服务
sudo /etc/init.d/ssh start

#分发密钥
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 600 ~/.ssh/authorized_keys
ssh-add
#出现提示:     Identity added: /home/qinyao/.ssh/id_rsa (/home/qinyao/.ssh/id_rsa)
#证明已经附加成功!

```

##### 配置

```shell
cd/home/lai/bigdata/hadoop/hadoop-3.3.0/etc/hadoop
vim hdfs-site.xml
```

1

```shell

<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/home/lai/bigdata/hadoop/temp/dfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/home/lai/bigdata/hadoop/temp/dfs/data</value>
    </property>
    <property>
        <name>dfs.http.address</name>
        <value>localhost:50070</value>
    </property>
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>localhost:50090</value>
    </property>
</configuration>
```

2

```shell
cd/home/lai/bigdata/hadoop/hadoop-3.3.0/etc/hadoop
vim core-site.xml

cd /home/wjh/bigdata/hadoop/hadoop-3.3.0/etc/hadoop
vi hadoop-env.sh 修改JAVA_HOME
```




```shell
<configuration>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/lai/bigdata/hadoop/temp</value>
        <description>Abase for other temporary directories.</description>
    </property>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
```

3

```shell
cd /home/lai/bigdata/hadoop/hadoop-3.3.0/etc/hadoop
vi hadoop-env.sh
```



```shell
export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true -Djava.security.krb5.realm= -Djava.security.krb5.kdc="
```



##### 格式化node

```shell
cd /home/lai/bigdata/hadoop/hadoop-3.3.0/bin
hdfs namenode -format
```

##### 启动

```shell
cd /home/lai/bigdata/hadoop/hadoop-3.3.0/sbin
# 配置了环境变量就不需要了
#host配置很关键！！！！！
start-dfs.sh
```

##### 停止

```shell
cd /home/lai/bigdata/hadoop/hadoop-3.3.0/bin
# 配置了环境变量就不需要了
stop-dfs.sh
```

